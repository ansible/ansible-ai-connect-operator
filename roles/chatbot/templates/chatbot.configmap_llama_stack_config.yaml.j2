---
apiVersion: v1
kind: ConfigMap
metadata:
  name: '{{ ansible_operator_meta.name }}-{{ deployment_type }}-llama-stack-config'
  namespace: '{{ ansible_operator_meta.namespace }}'
  labels:
    {{ lookup("template", "../common/templates/labels/common.yaml.j2") | indent(width=4) | trim }}
data:
  ansible-chatbot-run.yaml: |
    version: '2'
    image_name: ansible-chatbot
    container_image: ansible-chatbot
    apis:
      - inference
      - vector_io
      - safety
      - agents
      - datasetio
      - tool_runtime
      - files
    providers:
      inference:
        - provider_id: {{ chatbot_llm_provider_type }}
{% if chatbot_llm_provider_type == "openai" %}
          provider_type: remote::openai
          config:
            api_key: ${env.PROVIDER_TOKEN}
            base_url: ${env.PROVIDER_URL:=https://api.openai.com/v1}
{% elif chatbot_llm_provider_type == "azure_openai" %}
          provider_type: remote::azure
          config:
            api_key: ${env.PROVIDER_TOKEN}
            api_base: ${env.PROVIDER_URL}
            api_type: null
{% elif chatbot_llm_provider_type == "rhoai_vllm" %}
          provider_type: remote::vllm
          config:
            base_url: ${env.PROVIDER_URL}
            max_tokens: ${env.VLLM_MAX_TOKENS:=4096}
            api_token: ${env.PROVIDER_TOKEN:=fake}
            tls_verify: ${env.VLLM_TLS_VERIFY:=true}
{% endif %}
{% for key, value in chatbot_model_config_extras.items() %}
            {{ key }}: {{ value }}
{% endfor %}
        - provider_id: inline_sentence-transformer
          provider_type: inline::sentence-transformers
          config: {}
      vector_io:
        - provider_id: aap_faiss
          provider_type: inline::faiss
          config:
            persistence:
              namespace: vector_io::faiss
              backend: kv_rag
      safety:
        - provider_id: llama-guard
          provider_type: inline::llama-guard
          config:
            excluded_categories: []
      agents:
        - provider_id: lightspeed_inline_agent
          provider_type: inline::lightspeed_inline_agent
          config:
            persistence:
              agent_state:
                namespace: agents_state
                backend: kv_default
              responses:
                table_name: agents_responses
                backend: sql_default
            tools_filter:
              enabled: true
              model_id: ${env.INFERENCE_MODEL_FILTER:=}
              always_include_tools:
              - knowledge_search
{% for key, value in chatbot_agent_config_extras.items() %}
            {{ key }}: {{ value }}
{% endfor %}
      datasetio:
        - provider_id: localfs
          provider_type: inline::localfs
          config:
            kvstore:
              namespace: localfs_datasetio
              backend: kv_default
      files:
        - provider_id: meta-reference-files
          provider_type: inline::localfs
          config:
            storage_dir: ${env.PROVIDERS_DB_DIR:=/.llama/data/distributions/ansible-chatbot}/files
            metadata_store:
              table_name: files_metadata
              backend: sql_default
      tool_runtime:
        - provider_id: rag-runtime
          provider_type: inline::rag-runtime
          config: {}
        - provider_id: model-context-protocol
          provider_type: remote::model-context-protocol
          config: {}
    storage:
      backends:
        kv_rag:
          type: kv_sqlite
          db_path: ${env.VECTOR_DB_DIR:=/.llama/data/distributions/ansible-chatbot}/aap_faiss_store.db
        kv_default:
          type: kv_sqlite
          db_path: ${env.KV_STORE_DB_DIR:=/.llama/data/distributions/ansible-chatbot}/kv_store.db
        sql_default:
          type: sql_sqlite
          db_path: ${env.SQL_STORE_DB_DIR:=/.llama/data/distributions/ansible-chatbot}/sql_store.db
      stores:
        metadata:
          namespace: registry
          backend: kv_default
        inference:
          table_name: inference_store
          backend: sql_default
          max_write_queue_size: 10000
          num_writers: 4
        conversations:
          table_name: openai_conversations
          backend: sql_default
        prompts:
          namespace: prompts
          backend: sql_default
    registered_resources:
      models:
        - metadata: {}
          model_id: ${env.INFERENCE_MODEL}
          provider_id: {{ chatbot_llm_provider_type }}
          provider_model_id: null
        - metadata:
            embedding_dimension: 768
          model_id: sentence-transformers/all-mpnet-base-v2
          provider_id: inline_sentence-transformer
          provider_model_id: ${env.EMBEDDINGS_MODEL:=/.llama/data/embeddings_model}
          model_type: embedding
      shields: []
      vector_stores:
        - metadata: {}
          vector_store_id: ${env.PROVIDER_VECTOR_DB_ID:=}
          embedding_model: ${env.EMBEDDINGS_MODEL:=inline_sentence-transformer//.llama/data/embeddings_model}
          embedding_dimension: 768
          provider_id: "aap_faiss"
      datasets: []
      scoring_fns: []
      benchmarks: []
      tool_groups:
        - toolgroup_id: builtin::rag
          provider_id: rag-runtime
{% if _aap_gateway_url is defined and _aap_controller_url is defined %}
        - toolgroup_id: mcp::aap-controller
          provider_id: model-context-protocol
          mcp_endpoint:
            uri: http://localhost:8004/sse
{% endif %}
{% if _aap_gateway_url is defined %}
        - toolgroup_id: mcp::aap-lightspeed
          provider_id: model-context-protocol
          mcp_endpoint:
            url: http://localhost:8005/sse
{% endif %}
      safety:
        - default_shield_id: llama-guard
    vector_stores:
      default_provider_id: faiss
      default_embedding_model: # Define the default embedding model for RAG
        provider_id: inline_sentence-transformer
        model_id: /.llama/data/embeddings_model
    logging: null
    server:
      port: 8443
      tls_certfile: null
      tls_keyfile: null
      tls_cafile: null
      auth: null
      disable_ipv6: false
    external_providers_dir: ${env.EXTERNAL_PROVIDERS_DIR:=/.llama/providers.d}
